{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34e8ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import malnis\n",
    "from malnis import show\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2c23c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_id</th>\n",
       "      <th>text</th>\n",
       "      <th>query</th>\n",
       "      <th>cited</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1</td>\n",
       "      <td>BERT fine-tuning; multi-document summarization...</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>For instance, pre-trained sentence embedding m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>643</td>\n",
       "      <td>BERT fine-tuning; multi-document summarization...</td>\n",
       "      <td>Transfer and multi-task learning have traditio...</td>\n",
       "      <td>[45] have presented a joint many-task model wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>233</td>\n",
       "      <td>BERT fine-tuning; multi-document summarization...</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>Recently, Liu and Lapata [43] have developed a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>20</td>\n",
       "      <td>Text classification (Korde and Mahender, 2012;...</td>\n",
       "      <td>With the capability of modeling bidirectional ...</td>\n",
       "      <td>XLNet (Yang et al., 2019) learns bidirectional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>1751</td>\n",
       "      <td>Text classification (Korde and Mahender, 2012;...</td>\n",
       "      <td>Recently, pre-trained models have achieved sta...</td>\n",
       "      <td>0 (Sun et al., 2019) proposed a continual pret...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          reference_id                                               text  \\\n",
       "paper_id                                                                    \n",
       "204                  1  BERT fine-tuning; multi-document summarization...   \n",
       "204                643  BERT fine-tuning; multi-document summarization...   \n",
       "204                233  BERT fine-tuning; multi-document summarization...   \n",
       "855                 20  Text classification (Korde and Mahender, 2012;...   \n",
       "855               1751  Text classification (Korde and Mahender, 2012;...   \n",
       "\n",
       "                                                      query  \\\n",
       "paper_id                                                      \n",
       "204       We introduce a new language representation mod...   \n",
       "204       Transfer and multi-task learning have traditio...   \n",
       "204       Bidirectional Encoder Representations from Tra...   \n",
       "855       With the capability of modeling bidirectional ...   \n",
       "855       Recently, pre-trained models have achieved sta...   \n",
       "\n",
       "                                                      cited  \n",
       "paper_id                                                     \n",
       "204       For instance, pre-trained sentence embedding m...  \n",
       "204       [45] have presented a joint many-task model wi...  \n",
       "204       Recently, Liu and Lapata [43] have developed a...  \n",
       "855       XLNet (Yang et al., 2019) learns bidirectional...  \n",
       "855       0 (Sun et al., 2019) proposed a continual pret...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/clean_examples.csv\", index_col = 0)\n",
    "show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef0a4be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paper_id\n",
       "204    BERT fine-tuning; multi-document summarization...\n",
       "204    BERT fine-tuning; multi-document summarization...\n",
       "204    BERT fine-tuning; multi-document summarization...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text[204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49240384",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e81e3dd44d4a0eb6539a15736a6788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
      "\n",
      "['For instance, pre-trained sentence embedding models such as the Bidirectional Encoder Representations from Transformers (BERT) [19], Skip-Thoughts [20] and ELMo [21] have shown to be effective for learning universal sentences representations, which are useful for many natural language processing tasks including automatic text summarization methods [22].We combine the strengths of both multi-task learning and the pre-trained language model BERT [19] to encode the input documents and obtain their sentences representation.Recently, Liu and Lapata [43] have developed a general framework for both extractive and abstractive text summarization based on the Bidirectional Encoder Representations from Transformers (BERT) [19], where authors Lamsiyah et al.We explore transfer learning from BERT sentence embedding model [19] to improve sentence representation learning, which helps boost the performance of the proposed method.Therefore, with the success of distributed representations, pre-trained language models such as ELMo [21], BERT [19], Skip-Thoughts [20] and InferSent [44] have demonstrated promising benefits in learning universal sentence representations.The Bidirectional Encoder Representations from Transformers (BERT) model [19] is based on a multi-layer bidirectional transformer [50] with attention mechanisms.Then, the pre-trained model can be applied to a new natural language processing task by adding a few layers to the source model, such as text classification [52], question/answering systems [19] and automatic text summarization [43].Most hyper-parameters are the same as in the pre-training step [19], except for the batch size, number of training epochs and the learning rate.', 'To summarise, the overall obtained results on the three DUC (2002–2004) datasets demonstrated that fine-tuning the pre-trained BERT model on coLA, SST-2, RTE, MNLI and QNLI datasets allows transferring useful and suitable knowledge for extractive multi-document summarization.', 'For BERT single-task fine-tuning, the only new parameters are those of the new additional output layer related to the downstream task.', 'BERTBASE model is designed to embed a sentence into 768-dimensional vectors, while BERTLARGE provides sentence embeddings vectors of 1024 dimensions.', 'In the third approach, BERT is fine-tuned simultaneously on four natural language processing tasks.', 'We distinguish between two popular approaches for learning sentence embeddings, namely multi-task learning and language model pre-training.', 'It plays a key role in many natural language processing tasks, more specifically in text representations learning.', 'Following this success, other works have explored the potential of deep learning models for sentence representation learning.', 'In contrast to the existing methods, we introduce a simple, effective and unsupervised method for extractive generic multi-document summarization.', 'In the first step, we fine-tune BERT model using both single-task and multi-task learning from the GLUE benchmark datasets [23].', '[34] have introduced a neural multi-document summarization system (GBN-MDS) based on both graph convolutional networks and recurrent neural networks.', 'It assigns a low score to the sentence when it is redundant and a high score when it is novel.']\n",
      "\n",
      "max score: 0.09594095554254452\n",
      "******************************\n",
      "\n",
      "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.\n",
      "\n",
      "['[45] have presented a joint many-task model with growing depth in a single end-to-end model, which makes use of linguistic hierarchies to solve increasingly complex natural language processing tasks.', 'We fine-tune BERT model on supervised intermediate tasks from GLUE benchmark [23] using single-task and multi-task fine-tuning.', 'is the embedding vector of the sentence Si.', 'Hence, multi-task learning is an important paradigm to learn universal sentence representations.']\n",
      "\n",
      "max score: 0.15469612796312704\n",
      "******************************\n",
      "\n",
      "Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings. Our code is available at https://github.com/nlpyang/PreSumm\n",
      "\n",
      "['Recently, Liu and Lapata [43] have developed a general framework for both extractive and abstractive text summarization based on the Bidirectional Encoder Representations from Transformers (BERT) [19], where authors Lamsiyah et al.Then, the pre-trained model can be applied to a new natural language processing task by adding a few layers to the source model, such as text classification [52], question/answering systems [19] and automatic text summarization [43].Architecture of the original BERT model [43].', '1–19 The Author(s), DOI: 10.1177/0165551521990616 have introduced a novel-based BERT document-level encoder able to capture the semantics of a document and thus generate representations of its sentences.', 'The obtained results show that our method using the fine-tuned BERT model, considered as a sentence embedding model, has significantly outperformed the centroid method based on BOW representations for most evaluation measures.', 'Then, we provide a brief overview of the transfer learning methods applied in text representation learning.', 'In the rest of this section, we present in detail the two BERT fine-tuning methods.', 'and the centroid embedding vector CD !', 'Specifically, ROUGE-N (ROUGE-1, ROUGE-2, ROUGE-4) and ROUGE-L metrics aim to measure the content similarity between the generated summaries and their corresponding reference summaries (gold summaries).', 'In this article, we propose an unsupervised extractive method for multi-document summarization based on transfer learning from the fine-tuned BERT models.', 'We review the related work in section 2.']\n",
      "\n",
      "max score: 0.21717171227897678\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.\n",
      "\n",
      "['XLNet (Yang et al., 2019) learns bidirectional contexts by maximizing expected likelihood over all permutations of factorization order and uses a generalized autoregressive pretraining mechanism to overcome the pretrain-finetune discrepancy of BERT.', 'Many of them are based on RNNs.', 'Hyperparameter settings for BERT on GLUE datasets.']\n",
      "\n",
      "max score: 0.19512194652897097\n",
      "******************************\n",
      "\n",
      "Recently, pre-trained models have achieved state-of-the-art results in various language understanding tasks, which indicates that pre-training on large-scale corpora may play a crucial role in natural language processing. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entity, semantic closeness and discourse relations. In order to extract to the fullest extent, the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which builds and learns incrementally pre-training tasks through constant multi-task learning. Experimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several common tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.\n",
      "\n",
      "['0 (Sun et al., 2019) proposed a continual pretraining framework which builds and learns incrementally pretraining tasks through constant multi-task learning, to capture lexical, syntactic and semantic information from training corpora.', 'Text classification (Minaee et al., 2020) is one of the key tasks in natural language processing and has a wide range of applications, such as sentiment analysis, spam detection, tag suggestion, etc.', 'ERNIE 2.0 (Sun et al., 2019) proposed a continual pretraining framework which builds and learns incrementally pretraining tasks through constant multi-task learning, to capture lexical, syntactic and semantic information from training corpora.', 'These models have achieved substantial success in learning language representations.', 'Hyperparameter settings for BERT on GLUE datasets.', '(2016) use multi-task learning to train RNNs, utilizing the correlation between tasks to improve text classification performance.', 'A number of approaches have been proposed for text classification.']\n",
      "\n",
      "max score: 0.21804510779213082\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(malnis_dataset)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q, d, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tqdm(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m]), data\u001b[38;5;241m.\u001b[39mtext, data\u001b[38;5;241m.\u001b[39mcited):\n\u001b[0;32m----> 4\u001b[0m     s, m, d \u001b[38;5;241m=\u001b[39m \u001b[43mmalnis_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrouge-2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarting_summary\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(q)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/malnis_dataset/library/malnis_dataset/__init__.py:52\u001b[0m, in \u001b[0;36mfind_summary\u001b[0;34m(query, document, starting_summary, metric, component)\u001b[0m\n\u001b[1;32m     49\u001b[0m     current_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sentences) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#         print(\"sentences\", len(sentences))\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m         raw_scores \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     53\u001b[0m             rouge\u001b[38;5;241m.\u001b[39mget_scores(\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#             compute_score(\u001b[39;00m\n\u001b[1;32m     55\u001b[0m                 hyps \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(summary \u001b[38;5;241m+\u001b[39m [c]), \n\u001b[1;32m     56\u001b[0m                 refs \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m     57\u001b[0m             )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m sentences\n\u001b[1;32m     59\u001b[0m         ]\n\u001b[1;32m     60\u001b[0m         scores \u001b[38;5;241m=\u001b[39m [d[metric][component] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m raw_scores]\n\u001b[1;32m     61\u001b[0m         max_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(scores)\n",
      "File \u001b[0;32m~/malnis_dataset/library/malnis_dataset/__init__.py:53\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m     current_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sentences) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#         print(\"sentences\", len(sentences))\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         raw_scores \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 53\u001b[0m             \u001b[43mrouge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;43;03m#             compute_score(\u001b[39;49;00m\n\u001b[1;32m     55\u001b[0m \u001b[43m                \u001b[49m\u001b[43mhyps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m sentences\n\u001b[1;32m     59\u001b[0m         ]\n\u001b[1;32m     60\u001b[0m         scores \u001b[38;5;241m=\u001b[39m [d[metric][component] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m raw_scores]\n\u001b[1;32m     61\u001b[0m         max_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(scores)\n",
      "File \u001b[0;32m~/envs/malnis/lib/python3.8/site-packages/rouge/rouge.py:107\u001b[0m, in \u001b[0;36mRouge.get_scores\u001b[0;34m(self, hyps, refs, avg, ignore_empty)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(hyps) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(refs))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m avg:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_avg_scores(hyps, refs)\n",
      "File \u001b[0;32m~/envs/malnis/lib/python3.8/site-packages/rouge/rouge.py:120\u001b[0m, in \u001b[0;36mRouge._get_scores\u001b[0;34m(self, hyps, refs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n\u001b[1;32m    119\u001b[0m     fn \u001b[38;5;241m=\u001b[39m Rouge\u001b[38;5;241m.\u001b[39mAVAILABLE_METRICS[m]\n\u001b[0;32m--> 120\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclusive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclusive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     sen_score[m] \u001b[38;5;241m=\u001b[39m {s: sc[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats}\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_lengths:\n",
      "File \u001b[0;32m~/envs/malnis/lib/python3.8/site-packages/rouge/rouge.py:59\u001b[0m, in \u001b[0;36mRouge.<lambda>\u001b[0;34m(hyp, ref, **k)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRouge\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     DEFAULT_METRICS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-l\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     52\u001b[0m     AVAILABLE_METRICS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-3\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-4\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-5\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-l\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk:\n\u001b[0;32m---> 59\u001b[0m             \u001b[43mrouge_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrouge_l_summary_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     60\u001b[0m     }\n\u001b[1;32m     61\u001b[0m     DEFAULT_STATS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     62\u001b[0m     AVAILABLE_STATS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/envs/malnis/lib/python3.8/site-packages/rouge/rouge_score.py:389\u001b[0m, in \u001b[0;36mrouge_l_summary_level\u001b[0;34m(evaluated_sentences, reference_sentences, raw_results, exclusive, **_)\u001b[0m\n\u001b[1;32m    387\u001b[0m union \u001b[38;5;241m=\u001b[39m Ngrams(exclusive\u001b[38;5;241m=\u001b[39mexclusive)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref_s \u001b[38;5;129;01min\u001b[39;00m reference_sentences:\n\u001b[0;32m--> 389\u001b[0m     lcs_count, union \u001b[38;5;241m=\u001b[39m \u001b[43m_union_lcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluated_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mref_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mprev_union\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mexclusive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclusive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m     union_lcs_sum_across_all_references \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lcs_count\n\u001b[1;32m    395\u001b[0m llcs \u001b[38;5;241m=\u001b[39m union_lcs_sum_across_all_references\n",
      "File \u001b[0;32m~/envs/malnis/lib/python3.8/site-packages/rouge/rouge_score.py:333\u001b[0m, in \u001b[0;36m_union_lcs\u001b[0;34m(evaluated_sentences, reference_sentence, prev_union, exclusive)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eval_s \u001b[38;5;129;01min\u001b[39;00m evaluated_sentences:\n\u001b[1;32m    332\u001b[0m     evaluated_words \u001b[38;5;241m=\u001b[39m _split_into_words([eval_s])\n\u001b[0;32m--> 333\u001b[0m     lcs \u001b[38;5;241m=\u001b[39m \u001b[43m_recon_lcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluated_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclusive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclusive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     combined_lcs_length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(lcs)\n\u001b[1;32m    335\u001b[0m     lcs_union \u001b[38;5;241m=\u001b[39m lcs_union\u001b[38;5;241m.\u001b[39munion(lcs)\n",
      "File \u001b[0;32m~/envs/malnis/lib/python3.8/site-packages/rouge/rouge_score.py:177\u001b[0m, in \u001b[0;36m_recon_lcs\u001b[0;34m(x, y, exclusive)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mReturns the Longest Subsequence between x and y.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mSource: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m  sequence: LCS of x and y\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    176\u001b[0m i, j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x), \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[0;32m--> 177\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m_lcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recon\u001b[39m(i, j):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;124;03m\"\"\"private recon calculation\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/malnis/lib/python3.8/site-packages/rouge/rouge_score.py:157\u001b[0m, in \u001b[0;36m_lcs\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m j \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    156\u001b[0m     table[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m x[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m    158\u001b[0m     table[i, j] \u001b[38;5;241m=\u001b[39m table[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(malnis_dataset)\n",
    "\n",
    "for q, d, c in zip(tqdm(data[\"query\"]), data.text, data.cited):\n",
    "    s, m, d = malnis.find_summary(\n",
    "        q, \n",
    "        d, \n",
    "        metric = \"rouge-2\", \n",
    "        component = \"f\", \n",
    "        starting_summary = c\n",
    "    )\n",
    "    print(q)\n",
    "    print()\n",
    "    print(s)\n",
    "    print()\n",
    "    print(\"max score:\", m)\n",
    "#     print(d)\n",
    "    print(\"*\" * 30)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a499e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
